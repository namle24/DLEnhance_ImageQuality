name: train_realesrgan_siamese_improved
model_type: RealESRGANSiameseModel
num_gpu: 1
scale: 4
manual_seed: 0

datasets:
  train:
    name: TrainSiamese
    type: PairedSiameseImageDataset
    phase: train
    dataroot_gt: /datasets/RealESRGAN_data/dataset/train
    dataroot_lq_a: /datasets/RealESRGAN_data/dataset/train
    dataroot_lq_b: /datasets/RealESRGAN_data/dataset/train
    meta_info_file: /datasets/RealESRGAN_data/meta_info/meta_info_triplet_nosub.txt
    io_backend:
      type: disk
    gt_size: 256
    max_retry: 3
    use_hflip: true
    use_rot: true
    scale: 4
    # Remove normalization - keep images in [0,1] range
    # mean: [0.0, 0.0, 0.0]  # Commented out
    # std: [1.0, 1.0, 1.0]   # Commented out
    
    # Enhanced augmentation
    color_jitter: true
    blur_prob: 0.1
    noise_prob: 0.05
    
    # Dataloader settings
    num_workers: 4
    use_shuffle: true
    batch_size_per_gpu: 4  # Reduced for stability
    num_worker_per_gpu: 4
    dataset_enlarge_ratio: 1
    prefetch_mode: ~

  val:
    name: ValSet5  
    type: PairedImageDataset 
    dataroot_gt: /datasets/RealESRGAN_data/benchmark_datasets/Set5/Set5/GTmod12
    dataroot_lq: /datasets/RealESRGAN_data/benchmark_datasets/Set5/Set5/LRbicx4
    io_backend:
      type: disk

network_g:
  type: RRDBNetFeatOut
  num_in_ch: 3
  num_out_ch: 3
  scale: 4
  num_feat: 64
  num_block: 23
  num_grow_ch: 32
  return_intermediate: true 

network_d:
  type: UNetDiscriminatorSN
  num_in_ch: 3
  num_feat: 64

path:
  # Load pretrained Real-ESRGAN for initialization
  pretrain_network_g: /home/namlh/DLEnhance_ImageQuality/Real-ESRGAN/weights/RealESRGAN_x4plus.pth
  param_key_g: params_ema
  strict_load_g: false  # Allow partial loading
  resume_state: /home/namlh/DLEnhance_ImageQuality/Real-ESRGAN/experiments/train_realesrgan_siamese_improved/training_states/80000.state

train:
  ema_decay: 0.999
  
  # Improved optimizer settings
  optim_g:
    type: Adam
    lr: !!float 2e-5  # Lower learning rate for stability
    weight_decay: 0
    betas: [0.9, 0.999]
    
  optim_d:
    type: Adam
    lr: !!float 2e-5  # Match generator LR
    weight_decay: 0
    betas: [0.9, 0.999]

  scheduler:
    type: CosineAnnealingRestartLR
    periods: [200000, 200000]
    restart_weights: [1, 1]
    eta_min: !!float 1e-6

  total_iter: 400000
  warmup_iter: 5000  # Warmup period

  # Loss configuration
  pixel_opt:
    type: L1Loss
    loss_weight: 1.0
    reduction: mean

  perceptual_opt:
    type: PerceptualLoss
    layer_weights:
      'conv1_2': 0.1
      'conv2_2': 0.1  
      'conv3_4': 1.0
      'conv4_4': 1.0
      'conv5_4': 1.0
    vgg_type: vgg19
    use_input_norm: true
    range_norm: false
    perceptual_weight: 1.0
    style_weight: 0.25

  gan_opt:
    type: GANLoss
    gan_type: vanilla
    real_label_val: 1.0
    fake_label_val: 0.0
    loss_weight: !!float 1e-1

  # Enhanced Knowledge Distillation settings
  progressive_kd: true
  warmup_kd_iters: 50000
  lambda_kd_out: 0.2      # Increased from 0.05
  lambda_kd_feat: 0.1     # Increased from

logger:
  print_freq: 100
  save_checkpoint_freq: !!float 5e3
  use_tb_logger: true
  wandb:
    project: ~
    resume_id: ~

# Distributed training
dist_params:
  backend: nccl